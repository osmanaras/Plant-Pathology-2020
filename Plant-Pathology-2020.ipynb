{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate_distill_submission.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pytorch_lightning  as  pl\n",
    "from  pytorch_lightning . callbacks  import  EarlyStopping\n",
    "\n",
    "# Third party libraries\n",
    "import  torch\n",
    "from  scipy . special  import  softmax\n",
    "from  torch . utils . data  import  DataLoader\n",
    "from  tqdm  import  tqdm\n",
    "\n",
    "# User defined libraries\n",
    "from  dataset  import  generate_transforms , PlantDataset\n",
    "from  train  import  CoolSystem\n",
    "from  utils  import  init_hparams , init_logger , seed_reproducer , load_data\n",
    "\n",
    "\n",
    "if  __name__  ==  \"__main__\" :\n",
    "    # Make experiment reproducible\n",
    "    seed_reproducer ( 2020 )\n",
    "\n",
    "    # Init Hyperparameters\n",
    "    hparams  =  init_hparams ()\n",
    "\n",
    "    # init logger\n",
    "    logger  =  init_logger ( \"kun_out\" , log_dir = hparams . log_dir )\n",
    "\n",
    "    # Load data\n",
    "    data , test_data  =  load_data ( logger )\n",
    "\n",
    "    # Generate transforms\n",
    "    transforms  =  generate_transforms ( hparams . image_size ) # C'est ici qu'on fait le Data enhancement\n",
    "\n",
    "    early_stop_callback  =  EarlyStopping ( monitor = \"val_roc_auc\" , patience = 10 , mode = \"max\" , verbose = True )\n",
    "\n",
    "    # Instance Model, Trainer and train model\n",
    "    model  =  CoolSystem ( hparams ) # initialisation du model\n",
    "    trainer  =  pl . Trainer (       # entrainement du model \n",
    "        gpus = hparams . gpus ,\n",
    "        min_epochs = 70 ,\n",
    "        max_epochs = hparams . max_epochs ,\n",
    "        early_stop_callback = early_stop_callback ,\n",
    "        progress_bar_refresh_rate = 0 ,\n",
    "        precision = hparams . precision ,\n",
    "        num_sanity_val_steps = 0 ,\n",
    "        profiler = False ,\n",
    "        weights_summary = None ,\n",
    "        use_dp = True ,\n",
    "        gradient_clip_val = hparams . gradient_clip_val ,\n",
    "    ) \n",
    "\n",
    "    submission  = []\n",
    "    PATH  = [\n",
    "        \"logs_submit_distill/fold=0-epoch=59-val_loss=0.7352-val_roc_auc=0.9928.ckpt\" ,\n",
    "        \"logs_submit_distill/fold=1-epoch=28-val_loss=0.8069-val_roc_auc=0.9918.ckpt\" ,\n",
    "        \"logs_submit_distill/fold=2-epoch=28-val_loss=0.7605-val_roc_auc=0.9959.ckpt\" ,\n",
    "        \"logs_submit_distill/fold=3-epoch=66-val_loss=0.7628-val_roc_auc=0.9850.ckpt\" ,\n",
    "        \"logs_submit_distill/fold=4-epoch=32-val_loss=0.7845-val_roc_auc=0.9915.ckpt\" ,\n",
    "    ]\n",
    "\n",
    "    # ================================================ ================================================= ===========\n",
    "    # Test Submit\n",
    "    # ================================================ ================================================= ===========\n",
    "    \n",
    "    # chargement du test set\n",
    "    test_dataset  =  PlantDataset (\n",
    "        test_data , transforms = transforms [ \"train_transforms\" ], soft_labels_filename = hparams . soft_labels_filename\n",
    "    )\n",
    "    test_dataloader  =  DataLoader (\n",
    "        test_dataset , batch_size = 64 , shuffle = False , num_workers = hparams . num_workers , pin_memory = True , drop_last = False ,\n",
    "    )\n",
    "\n",
    "    # chargement du model entrainé\n",
    "    for  path  in  PATH :\n",
    "        model . load_state_dict ( torch . load ( path )[ \"state_dict\" ])\n",
    "        model . to ( \"cuda\" )\n",
    "        model . eval ()\n",
    "        \n",
    "        # prediction du test ser\n",
    "        for  i  in  range ( 8 ):\n",
    "            test_preds  = []\n",
    "            labels  = []\n",
    "            with  torch . no_grad ():\n",
    "                for  image , label , times  in  tqdm ( test_dataloader ):\n",
    "                    test_preds . append ( model ( image . to ( \"cuda\" )))  # prediction\n",
    "                    labels . append ( label )\n",
    "\n",
    "                labels  =  torch . cat ( labels )\n",
    "                test_preds  =  torch . cat ( test_preds )\n",
    "                submission . append ( test_preds . cpu (). numpy ())\n",
    "\n",
    "    submission_ensembled  =  0\n",
    "    for  sub  in  submission :\n",
    "        submission_ensembled  +=  softmax ( sub , axis = 1 ) /  len ( submission )\n",
    "    test_data . iloc [:, 1 :] =  submission_ensembled\n",
    "    test_data . to_csv ( \"submission_distill.csv\" , index = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pretrainedmodels\n",
    "\n",
    "# definition de la fonction  de normalisation à utiliser \n",
    "def l2_norm(input, axis=1):\n",
    "    norm = torch.norm(input, 2, axis, True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "class BinaryHead(nn.Module):\n",
    "    def __init__(self, num_class=4, emb_size=2048, s=16.0):\n",
    "        super(BinaryHead, self).__init__()\n",
    "        self.s = s\n",
    "        self.fc = nn.Sequential(nn.Linear(emb_size, num_class))\n",
    "\n",
    "    def forward(self, fea):\n",
    "        fea = l2_norm(fea)\n",
    "        logit = self.fc(fea) * self.s\n",
    "        return logit\n",
    "\n",
    "\n",
    "# definition du gros model (knowledge distillation)\n",
    "class se_resnext50_32x4d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(se_resnext50_32x4d, self).__init__()\n",
    "        # chargement du pre-trained model\n",
    "        self.model_ft = nn.Sequential(\n",
    "            *list(pretrainedmodels.__dict__[\"se_resnext50_32x4d\"](num_classes=1000, pretrained=\"imagenet\").children())[\n",
    "                :-2\n",
    "            ]\n",
    "        )\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # pooling layer \n",
    "        self.model_ft.last_linear = None\n",
    "        self.fea_bn = nn.BatchNorm1d(2048)\n",
    "        self.fea_bn.bias.requires_grad_(False)\n",
    "        self.binary_head = BinaryHead(4, emb_size=2048, s=1)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        img_feature = self.model_ft(x) # appel du pre-trained model\n",
    "        img_feature = self.avg_pool(img_feature) # pooling layers\n",
    "        img_feature = img_feature.view(img_feature.size(0), -1) \n",
    "        fea = self.fea_bn(img_feature) # premiere normalisation\n",
    "        # fea = self.dropout(fea)\n",
    "        output = self.binary_head(fea) # deuxieme normalisation\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los_function.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition de la loss function\n",
    "class CrossEntropyLossOneHot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLossOneHot, self).__init__()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1) # log(softmax(x))\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        return torch.mean(torch.sum(-labels * self.log_softmax(preds), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition du petit model\n",
    "class  CoolSystem ( pl . LightningModule ):\n",
    "    def  __init__ ( self , hparams ):\n",
    "        super (). __init__ ()\n",
    "        self . hparams  =  hparams\n",
    "\n",
    "        # Make the model initialization consistent every time, so as long as there is a re-initialization in the middle, the result will immediately go wrong\n",
    "        seed_reproducer ( self . hparams . seed )\n",
    "\n",
    "        self . model  =  se_resnext50_32x4d () # initialisation du pre-trained model\n",
    "        self . criterion  =  CrossEntropyLossOneHot () # initialisation de la loss\n",
    "        self . logger_kun  =  init_logger ( \"kun_in\" , hparams . log_dir ) # initialisation du log file\n",
    "\n",
    "    # initialise le model lors de l'apperl de la classe\n",
    "    def  forward ( self , x ):\n",
    "        return  self . model ( x )\n",
    "\n",
    "    # initialisation de l'optimizer\n",
    "    def  configure_optimizers ( self ):\n",
    "        # the optimize\n",
    "        self . optimizer  =  torch . optim . Adam ( self . parameters (), lr = 0.001 ,\n",
    "                                                   betas = ( 0.9 , 0.999 ), eps = 1e-08 ,\n",
    "                                                   weight_decay = 0 )\n",
    "        # function who optimize \n",
    "        self . scheduler  =  WarmRestart ( self . optimizer , T_max = 10 ,\n",
    "                                          T_mult = 1 , eta_min = 1e-5 )\n",
    "        return [ self . optimizer ], [ self . scheduler ]\n",
    "\n",
    "    # entrainement du model -> Training loop\n",
    "    def  training_step ( self , batch , batch_idx ):\n",
    "        step_start_time  =  time ()\n",
    "        images , labels , data_load_time  =  batch\n",
    "\n",
    "        scores  =  self ( images )\n",
    "        loss  =  self . criterion ( scores , labels )\n",
    "        # self.logger_kun.info(f\"loss: {loss.item()}\")\n",
    "        #! can only return scalar tensor in training_step\n",
    "        # must return key -> loss\n",
    "        # optional return key -> progress_bar optional (MUST ALL BE TENSORS)\n",
    "        # optional return key -> log optional (MUST ALL BE TENSORS)\n",
    "        data_load_time  =  torch . sum ( data_load_time )\n",
    "\n",
    "        return {\n",
    "            \"loss\" : loss ,\n",
    "            \"data_load_time\" : data_load_time ,\n",
    "            \"batch_run_time\" : Torch . Tensor ([ Time () -  step_start_time  +  data_load_time ]). to ( data_load_time . Device ),\n",
    "        }\n",
    "\n",
    "    def  training_epoch_end ( self , outputs ):\n",
    "        # outputs is the return of training_step\n",
    "        train_loss_mean  =  torch . stack ([ output [ \"loss\" ] for  output  in  outputs ]). mean ()\n",
    "        self . data_load_times  =  torch . stack ([ output [ \"data_load_time\" ] for  output  in  outputs ]). sum ()\n",
    "        self . batch_run_times  =  torch . stack ([ output [ \"batch_run_time\" ] for  output  in  outputs ]). sum ()\n",
    "\n",
    "        self . current_epoch  +=  1\n",
    "        \n",
    "        if  self . current_epoch  < ( self . trainer . max_epochs  -  4 ):\n",
    "            self . scheduler  =  warm_restart ( self . scheduler , T_mult = 2 )\n",
    "\n",
    "        return { \"train_loss\" : train_loss_mean }\n",
    "    \n",
    "    \n",
    "    # test du model  -> Evaluating Loop \n",
    "    def  validation_step ( self , batch , batch_idx ):\n",
    "        step_start_time  =  time ()\n",
    "        images , labels , data_load_time  =  batch\n",
    "        data_load_time  =  torch . sum ( data_load_time )\n",
    "        scores  =  self ( images )\n",
    "        loss  =  self . criterion ( scores , labels )\n",
    "\n",
    "        # must return key -> val_loss\n",
    "        return {\n",
    "            \"val_loss\" : loss ,\n",
    "            \"scores\" : scores ,\n",
    "            \"labels\" : labels ,\n",
    "            \"data_load_time\" : data_load_time ,\n",
    "            \"batch_run_time\" : Torch . Tensor ([ Time () -  step_start_time  +  data_load_time ]). to ( data_load_time . Device ),\n",
    "        }\n",
    "\n",
    "    def  validation_epoch_end ( self , outputs ):\n",
    "        # compute loss\n",
    "        val_loss_mean  =  torch . stack ([ output [ \"val_loss\" ] for  output  in  outputs ]). mean ()\n",
    "        self . data_load_times  =  torch . stack ([ output [ \"data_load_time\" ] for  output  in  outputs ]). sum ()\n",
    "        self . batch_run_times  =  torch . stack ([ output [ \"batch_run_time\" ] for  output  in  outputs ]). sum ()\n",
    "\n",
    "        # compute roc_auc\n",
    "        scores_all  =  torch . cat ([ output [ \"scores\" ] for  output  in  outputs ]). cpu ()\n",
    "        labels_all  =  torch . round ( torch . cat ([ output [ \"labels\" ] for  output  in  outputs ]). cpu ())\n",
    "        val_roc_auc  =  roc_auc_score ( labels_all , scores_all )\n",
    "\n",
    "        # terminal logs\n",
    "        self . logger_kun . info (\n",
    "            f\" { self . hparams . fold_i } - { self . current_epoch } | \"\n",
    "            f\"lr: { self . scheduler . get_lr ()[ 0 ]:.6f } | \"\n",
    "            f\"val_loss: { val_loss_mean :.4f } | \"\n",
    "            f\"val_roc_auc: { val_roc_auc :.4f } | \"\n",
    "            f\"data_load_times: { self . data_load_times :.2f } | \"\n",
    "            f\"batch_run_times: { self . batch_run_times :.2f } \"\n",
    "        )\n",
    "        # f\"data_load_times: {self.data_load_times:.2f} | \"\n",
    "        # f\"batch_run_times: {self.batch_run_times:.2f}\"\n",
    "        # must return key -> val_loss\n",
    "        return { \"val_loss\" : val_loss_mean , \"val_roc_auc\" : val_roc_auc }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
